---
title: multidimensional scaling of variables
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CMDS of covariances}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  fig.height = 6, fig.align = "center"
)
```

This vignette demonstrates that covariance (and correlation) data can be summarized and visualized using multidimensional scaling using the same principles and machinery as for distance (or similarity) data.
I argue that these geometric representations have interpretative advantages over other methods and should be more widely used.

## dimension reduction of geometric data

Because ordination is grounded in geometric data analysis, it is easiest and most straightforward on naturally geometric data. For example, the glucose and insulin data collected by [Reaven and Miller (1970)](https://link.springer.com/article/10.1007/BF00423145) consists of five continuous-valued measurements for each of 145 persons. Because the variables (measurements) have different scales, for purposes of representing the cases (persons) it makes sense to separately center and scale them. The most basic ordination method, principal components analysis (PCA), proceeds to compute a singular value decomposition $X=UDV^\top$ of the centered and scaled data matrix $X\in\mathbb{R}^{n\times p}$:

```{r diabetes pca}
x <- scale(heplots::Diabetes[, -6L], center = TRUE, scale = TRUE)
s <- svd(x)
r <- length(s$d)
```

The matrix factors $U\in\mathbb{R}^{n\times r}$ and $V\in\mathbb{R}^{p\times r}$ arise from eigendecompositions of $XX^\top$ and of $X^\top X$, respectively, which have the same set of eigenvalues $\lambda_1,\ldots,\lambda_r$. The square roots of these eigenvalues make up the diagonal matrix $D\in\mathbb{R}^{r\times r}$. The conventional biplot of this PCA uses _principal coordinates_ for the cases, obtained from the rows of $UD$, and _standardized coordinates_ for the variables, obtained from the rows of $V$.

Both sets of eigenvectors $U=\left[\,u_1\,\cdots\,u_r\,\right]$ and $V=\left[\,v_1\,\cdots\,v_r\,\right]$ are orthonormal, which means that $U^\top U=I_r=V^\top V$ and that the total variance (called _inertia_) in each matrix is $\sum_{j=1}^{r}{ {v_j}^2 }=r=\sum_{j=1}^{r}{ {v_j}^2 }$. Meanwhile, the diagonal $D$ contains all of the inertia of the centered and scaled data matrix $X$:

```{r inertia}
# inertia of the (scaled) data
sum(x^2)
# inertia of the case and variable factors
sum(s$u^2)
sum(s$v^2)
# inertia of the diagonal factor
sum(s$d^2)
```

For the (2-dimensional) biplot, this inertia is _conferred_ on the cases (converting standardized coordinates to principal coordinates) so that the planar distances between the points in the plot representing the cases are approximately equal to their Euclidean distances in $X$. (The relationship is approximate because the biplot only accommodates two of the four principal components.)
Here is a scatterplot of the true distances and their biplot approximations:

```{r case geometry, fig.width=5.5}
# distances between cases
x.dist <- dist(x)
# distances between cases (principal coordinates)
s.dist <- dist(s$u[, 1:2] %*% diag(s$d[1:2]))
# scatterplot
plot(
  x = as.vector(x.dist), y = as.vector(s.dist),
  xlim = c(0, 8), ylim = c(0, 8),
  asp = 1, pch = 19, cex = .5,
  xlab = "Case distances in centered and scaled data",
  ylab = "Case point distances in planar biplot"
)
lines(x = c(0, 8), y = c(0, 8))
```

In contrast, the variables are better understood through their correlations, which are approximately preserved by their standardized coordinates.
Writing $X=[\,y_1\,\cdots\,y_m\,]$ as an array of column variables, the covariance between $y_i$ and $y_j$ is proportional to their inner product $$\textstyle \operatorname{cov}(y_i,y_j)=\frac{1}{n}y_i\cdot y_j=\frac{1}{n}\lVert y_i\rVert\lVert y_j\rVert\cos\theta_{ij}\text,$$ so that the cosine of the angle $\theta_{ij}$ between them equals their correlation:
$$\cos\theta_{ij}=\frac{\operatorname{cov}(y_i,y_j)}{\sqrt{\operatorname{cov}(y_i,y_i)\operatorname{cov}(y_j,y_j)}/n}=\frac{\operatorname{cov}(y_i,y_j)}{\sigma_i\sigma_j}=r_{ij}$$
Here the cosines $\frac{g_i}{\lVert g_i\rVert}\cdot\frac{g_j}{\lVert g_j\rVert}$ between the variable vectors in the biplot are plotted against their correlations $r_{ij}$ in the centered and scaled data:

```{r variable geometry, fig.width=5.5}
# correlations between variables
x.cor <- cor(x)
# magnitudes of variable vectors
s.len <- apply(s$v[, 1:2] %*% diag(s$d[1:2]), 1, norm, "2")
# cosines between variables (principal coordinates)
s.cor <- (s$v[, 1:2] / s.len) %*% diag(s$d[1:2]^2) %*% t(s$v[, 1:2] / s.len)
# scatterplot
plot(
  x = as.vector(x.cor[lower.tri(x.cor)]),
  y = as.vector(s.cor[lower.tri(s.cor)]),
  xlim = c(-1, 1), ylim = c(-1, 1),
  asp = 1, pch = 19, cex = .5,
  xlab = "Variable correlations in centered and scaled data",
  ylab = "Variable vector cosines in planar biplot"
)
lines(x = c(-1, 1), y = c(-1, 1))
```



## multidimensional scaling of distance data

The faithful approximation of inter-case distances by principal coordinates is the idea behind _classical multidimensional scaling_ (CMDS), which can be applied to a data set of distances $\delta_{ij},\ 1\leq i,j\leq n$ in the absence of coordinates (variables). CMDS produces a set of $r$ artificial coordinates for the cases that yield a best approximation of the inter-case distances in terms of the sum of squared errors. The technique uses the eigendecomposition of a doubly-centered matrix of squared distances, which produces a matrix $U\Lambda^{1/2}$ whose first $r$ coordinates---for any $r\leq n$---minimize the variance of $(U\Lambda^{1/2})(U\Lambda^{1/2})^\top-\Delta=U\Lambda U^\top-\Delta$, where $\Delta=(\delta_{ij})\in\mathbb{R}^{n\times n}$, to obtain the approximation. In practice, the goal is usually to position points representing the $n$ cases in a 2-dimensional scatterplot so that their distances $\sqrt{(x_j-x_i)^2+(y_j-y_i)^2}$ approximate their original distances $\delta_{ij}$, as in this example using road distances between U.S. cities to approximate their geographic arrangement:

```{r multidimensional scaling, fig.height=5, fig.width=7}
d <- as.matrix(UScitiesD)
cent <- diag(1, nrow(d)) - matrix(1/nrow(d), nrow(d), nrow(d))
d.cent <- -.5 * cent %*% (d^2) %*% cent
d.cmds <- svd(d.cent)
d.coord <- d.cmds$u[, 1:2] %*% diag(sqrt(d.cmds$d[1:2]))
plot(d.coord, pch = NA, asp = 1, xlab = "", ylab = "")
text(d.coord, labels = rownames(d))
```



## multidimensional scaling of covariance data

The faithful approximation of inter-variable correlations by the angles between their coordinate vectors provides a dual form of CMDS. Suppose we have data that consist not of distances between cases but of covariances $\operatorname{cov}(y_i,y_j),\ 1\leq i,j\leq p$ between variables. Again the data are coordinate-free, so PCA is inapplicable. But were the data to have derived from a centered case--variable matrix $X$, then the covariance matrix $C=(\operatorname{cov}(y_i,y_j))$ would have been obtained as $C=\frac{1}{n}X^\top X$. This, up to scalar, is the matrix whose eigenvectors would be given by $V$ in the SVD $X=UDV^\top$. Therefore, we can obtain artificial coordinates for these variables that approximate what we know of their geometry---in this case, thinking of the variables as unknown vectors whose magnitudes and angles are encoded in $C$---via an eigendecomposition $C=V\Lambda V^\top$: Take $Y=V\Lambda^{1/2}\in\mathbb{R}^{p\times r}$, so that $Y^\top Y\approx C$.

I'll validate this line of reasoning on the diabetes data set:

```{r diabetes example}
# covariances and standard deviations
c <- cov(heplots::Diabetes[, -6L])
s <- diag(sqrt(diag(c)))
# centered data
x <- as.matrix(scale(mtcars, center = TRUE, scale = FALSE))
# eigendecomposition of covariance matrix
c.eigen <- eigen(c)
# artificial coordinates
c.coord <- c.eigen$vectors %*% diag(sqrt(c.eigen$values))
# validate covariance recovery (up to sign)
all.equal(
  as.vector(c.coord %*% t(c.coord)),
  as.vector(c),
  tolerance = 1e-12
)
```

Thus, whereas _CMDS of cases_ represents distances, _CMDS of variables_ represents covariances, in both cases in low dimensions.
The natural use case for this technique is a situation in which covariance data exist without variable values, for example if original data are sensitive or inaccessible.

A more interesting setting that gives rise to this situation is the analysis of multiple rankings of the same set of objects in terms of their _concordance_. Rankings' concordance is often measured using rank correlations such as Kendall's $\tau$, which may be _general correlation coefficients_ in [the sense proposed by Kendall](https://en.wikipedia.org/wiki/Rank_correlation#General_correlation_coefficient) but are not associated with an underlying geometry. Nevertheless, we can use CMDS to represent these rankings as unit vectors in Euclidean space whose pairwise cosines approximate their rank correlations!

## example: rankings of universities

A real-world example is provided by the [Quacquarelli Symonds World University Rankings](https://www.topuniversities.com/qs-world-university-rankings/methodology), which include rankings of hundreds of world universities along six dimensions: academic reputation, employer reputation, faculty--student ratio, citations per faculty, international faculty ratio, and international student ratio. QS weight these rankings differently in their overall assessment, but our analysis will compare the rankings to each other across universities, ignoring these weights.

```{r QS top university rankings}
devtools::load_all()
data(qswur_usa)
print(qswur_usa)
```

For this example, i'll focus only on rankings for the year 2020 for which all integer rankings were available. This leaves me with only 38 universities, so my conclusions must be taken with caution!
Since the integer rankings were subsetted from the full international data set, they are not contiguous (i.e. some integers between rankings never appear). To resolve this, i'll also recalibrate the rankings by matching each vector of ranks to the vector of its sorted unique values:

```{r calibrate rankings}
library(dplyr)
qswur_usa %>%
  filter(year == 2020L) %>%
  select(institution, starts_with("rk_")) %>%
  mutate_at(
    vars(starts_with("rk_")),
    ~ match(., sort(unique(as.numeric(.))))
  ) %>%
  filter_at(vars(starts_with("rk_")), ~ ! is.na(.)) ->
  qswur_usa2020
print(qswur_usa2020)
```

This subset of universities is now contiguously ranked along the six dimensions described above. The Kendall correlation $\tau_{ij}$ between two rankings measures their concordance. To calculate it, every pair of universities contributes either $+1$ or $-1$ according as the rankings $i$ and $j$ place that pair in the same order, and the sum is scaled down by the number of pairs ${n\choose 2}$ so that the result lies between $-1$ and $1$. We interpret $\tau_{ij}=1$ as perfect concordance (the rankings are equivalent), $\tau_{ij}=-1$ as perfect discordance (the rankings are reversed), and $\tau_{ij}=0$ as independence (the rankings are unrelated).

The QS rankings are not variations on a theme, like different measures of guideline adherence or positive affect, but they do all seem potentially sensitive to a university's resources, including funding and prestige. I intuit that the two reputational metrics should be positively correlated, and that the two international ratios should be as well. I also wonder if the faculty--student ratio might be anti-correlated with the number of citations per faculty, separating more research-focused institutions from more teaching-focused ones.

### correlation heatmap

A common way to visualize correlation matrices is the heatmap, so i'll use that technique first (see below). While the rankings by academic and employer reputations are highly concordant, those by international faculty and student ratios are less so; and, the faculty--student ratio and faculty citation rankings have the weakest concordance of all, but are nevertheless positively correlated.

```{r Kendall rank correlations, fig.width=7}
corr <- cor(select(qswur_usa2020, starts_with("rk_")), method = "kendall")
corrplot::corrplot(corr)
```

This visualization is useful, but it's very busy: To compare any pair of rankings, i have to find the cell in the grid corresponding to that pair and refer back to the color scale to assess its meaning. I can't rely on the nearby cells for context, because they may be stronger or weaker than average and skew my interpretation. For example, the visibly weak associations between the faculty--student ratio and other rankings (the third row or column) happen to be arranged so that the slightly stronger among them, with the two reputational variables, are sandwiched between the _even stronger_ associations between the two reputational rankings and between them and the faculty citations ranking; whereas its weaker associations are sandwiched between more typical, but still comparatively stronger, associations. A different ordering of the variables might "obscure" this pattern and "reveal" others.

The plot is also strictly pairwise: Every correlation between two rankings occupies its own cell---two, in fact, making almost half of the plot duplicative. This means that a subset analysis of, say, three rankings requires focusing on three cells at the corners of a right triangle while ignoring all the surrounding cells. This is not an easy visual task. It would be straightforward to create a new plot for any subset, but then the larger context of the remaining rankings would be lost.

### correlation biplot

CMDS of variables offers a natural alternative visualization: the biplot. As with CMDS of cases, the point isn't to overlay the case scores and variable loadings from a singular value decomposition, but to use the scores or loadings alone to endow the cases or variables with a Euclidean geometry they didn't yet have. To that end, i'll plot the variables as vectors with tails at the origin and heads at their fabricated coordinates $Y=V\Lambda^{1/2}$:

```{r multidimensional scaling of variables, fig.width=7}
corr.eigen <- eigen(corr)
corr.coord <- corr.eigen$vectors %*% diag(sqrt(corr.eigen$values))
plot(corr.coord, pch = NA, asp = 1, xlab = "", ylab = "")
arrows(0, 0, corr.coord[, 1], corr.coord[, 2])
text(corr.coord, labels = rownames(corr))
```

A more elegant ggplot2-style graphic can be rendered with [ordr](https://github.com/corybrunson/ordr), with a unit circle included for reference:

```{r, fig.width=7}
eigen_ord(corr) %>%
  as_tbl_ord() %>%
  augment_ord() %>%
  mutate_rows(metric = stringr::str_remove(.name, "rk_")) %>%
  confer_inertia(1) ->
  c_eigen
c_eigen %>%
  ggbiplot() +
  theme_minimal() +
  geom_unit_circle() +
  geom_rows_vector() +
  geom_rows_text_radiate(aes(label = metric)) +
  scale_x_reverse(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .2)) +
  ggtitle("CMDS of Kendall correlations between university rankings")
```

With respect to the pairwise correlations, the biplot is significantly less precise: Though the vectors all have unit length in $\mathbb{R}^r$ ($r\leq p=6$), their projections onto the first two principal coordinates are much shorter, indicating that much of the geometric configuration requires additional dimensions to represent. Indeed, these coordinates capture only $48.2\%+14.3\%=62.5\%$ of the inertia in the full representation. This means that the angles between the vectors must be interpreted with caution: For example, it looks like the academic and employer reputation rankings are extremely correlated, but the apparent alignment of the vectors could be an artifact of the projection, when in fact they "rise" and "fall" in opposite directions along the remaining dimensions. The correlation heatmap leaves no such ambiguity.

However, the biplot far surpasses the heatmap at parsimony: Each variable is represented by a single vector, and the angle cosines between the variable vectors roughly approximate their correlations. For instance, the rankings based on international student and faculty ratios have correlation around $\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}$, corresponding to either explaining half the "variance" in the other---not technically meaningful in the ranking context but a useful conceptual anchor. Meanwhile, the faculty--student ratio ranking is nearly independent of the faculty citation ranking, contrary to my intuition that these rankings would reflect a _reverse_ association between research- and teaching-oriented institutions. The convenience of recognizing correlations as cosines may be worth the significant risk of error, especially since that error (the residual $37.5\%$ of inertia) can be exactly quantified.

Moreover, the principal coordinates of the variable vectors indicate their loadings onto the first and second principal moments of inertia---the two dimensions that capture the most variation in the data. For example, the first principal coordinate is most aligned with the two reputational rankings, suggesting that a general prestige ranking is the strongest overall component of the several specific rankings. In contrast, the faculty--student ratio and faculty citation rankings load most strongly onto the second principal coordinate, suggesting that the divide between research- and teaching-focused institutions may yet be important to understanding how universities compare along these different metrics. These observations, provisional though they are, would be difficult to discern from the heatmap. More importantly, unlike the secondary patterns visible in the heatmap, these are in no sense artifacts of the layout but arise directly from the (correlational) data.

This last point means that observations made from a biplot can be validated from the CMDS coordinates. In particular, we can examine the variables' loadings onto the third principal coordinate, and we can check whether the reputational rankings are aligned or misaligned along it.

```{r}
c_eigen %>%
  fortify(.matrix = "u") %>%
  select(-.name, -.matrix)
c_eigen %>%
  fortify(.matrix = "coord")
```

Based on the third principal coordinates, the reputational rankings are aligned, as we knew already from the correlation matrix and heatmap. What's a bit more interesting is that this component seems to separate these two rankings from those having to do with faculty citation rates and the international compositions of the faculty and student body. Based on the decomposition of inertia, this third principal coordinate is nearly as important as the second! It therefore makes sense to plot the two together:

```{r, fig.width=7}
c_eigen %>%
  ggbiplot(aes(x = 2, y = 3)) +
  theme_minimal() +
  geom_unit_circle() +
  geom_rows_vector() +
  geom_rows_text_radiate(aes(label = metric)) +
  scale_x_continuous(expand = expand_scale(add = .4)) +
  scale_y_continuous(expand = expand_scale(add = .4)) +
  ggtitle("CMDS of Kendall correlations between university rankings",
          "Second and third principal coordinates")
```

The primary antitheses of the reputational rankings, after removing the first principal coordinate, are the two rankings based on international composition---and this axis is largely independent of the axis apparently distinguishing research- from teaching-oriented institutions. From my own limited knowledge, i'd hazard a guess that this reflects two tiers of international representation among students and faculty, one expressed by the most prestigious institutions that recruit highly qualified applicants from all over the world, and the other expressed by institutions that are not especially prestigious but are located in communities or regions with high percentages of international residents.

This is of course no more than idle speculation on my part! But a visualization scheme that encourages hypothesis generation is worth having on hand.

## Notes

Adapted from this blog post: <https://corybrunson.github.io/2019/08/16/rank-correlations/>

"In multidimensional scaling, you represent distances between multidimensional objects using a smaller number of dimensions, typically two or three."[^1]

"We have $n$ objects and a matrix of distances between each object; $d_{rs}$ is the distance between objects $r$ and $s$ and $D = (d_{ij}) \in \mathbb{R}^{n \times n}$ is the distance matrix. Multidimensional scaling (CMDS) seeks to create points $x_1,\ldots,x_n\in\mathbb{R}^k$ such that $d_{rs} \approx \lVert x_r - x_s \rVert$."[^2]

[^1]: https://math.unm.edu/~james/w13-STAT576c.pdf
[^2]: https://people.stat.sc.edu/hansont/stat730/chapter14.pdf
